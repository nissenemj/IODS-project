# Työ 4

# Exercise 4

---
title: "Exercise 4"
author: "Mette Nissen"
date: "11/24/2019"
output: html_document
---

```{r}
# access packages

library(MASS)
library(ggplot2)
library(GGally)
library(tidyverse)
library(dplyr)
library(knitr)
library(corrplot)
library(tidyr)
library(reshape2)
library(plotly)


# load the data
data("Boston")
```

## Summary of the Boston Data

**This data frame contains the following columns:**
*crim* = per capita crime rate by town.
*zn* = proportion of residential land zoned for lots over 25,000 sq.ft.
*indus* = proportion of non-retail business acres per town.
*chas* = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
*nox* = nitrogen oxides concentration (parts per 10 million).
*rm* = average number of rooms per dwelling.
*age* = proportion of owner-occupied units built prior to 1940.
*dis* = weighted mean of distances to five Boston employment centres.
*rad* = index of accessibility to radial highways.
*tax* = full-value property-tax rate per \$10,000.
*ptratio* = pupil-teacher ratio by town.
*black* = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
*lstat* = lower status of the population (percent).
*medv* = median value of owner-occupied homes in \$1000s.

In this weeks exercise  we use Boston data set from R MASS package which is a histoical data collected from 606 districts in the area around Boston. 

Boston has 14 variables and 506 observations. Crime variable is the response variable. 

Variables and their explanations are show above. 


```{r Boston}

#Dataset summary and variables 

summary(Boston)
str(Boston)
colnames(Boston)
pairs(Boston)
head(Boston)

#Graphical summary of crime variable

ggplot(Boston, aes(crim)) + stat_density() + theme_bw()

#Plotting each variable against crime rate

bosmelt <- melt(Boston, id="crim")
ggplot(bosmelt, aes(x=value, y=crim))+
  facet_wrap(~variable, scales="free")+
  geom_point()

```


## Graphical overview and summaries of variables


```{r}

boxplot(Boston$crim, Boston$zn, Boston$indus, Boston$chas, Boston$nox, Boston$rm, Boston$age, Boston$dis, Boston$rad, Boston$tax, Boston$ptratio, Boston$black, Boston$lstat, Boston$medv, names = c("crim", "zn", "indus", "chas", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv"))

```

## Making multible linear Regression model with all variables

```{r MRM}
mlm <- lm(formula = crim ~ ., data = Boston)
summary(mlm)
```

### Multiple linear regression model intepratation

Most significant variables in the model are dis and rad with high significance, median value with moderate significance and zone, black with lower but still under *p* 0.05 significance. 

## Including Correlations

```{r correlation}
cor_matrix<-cor(Boston) 
cor_matrix %>% round(digits = 2)
corrplot.mixed(cor_matrix, number.cex = .6)
```

Corrplot shows the relationships between variables. Highest positive correlation are between rad and tax, indus and nox and age and nox. Highest negative correlations are between age and dis, lstat and med and dis and nox.
Wee can see from the summaries that distribution of the variables is very inconsistent and thus we need to scale the dataset before doing linear discriminant analysis later.

# Standardizing the dataset

```{r standardize and summary}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

summary(boston_scaled)
```

### Scaling effects

With standardizing data is centralized. This is done to continuous variables on unit scale by subtracting the mean of the variable and dividing the result by the variable's standard deviation. With this variables´mean is 0 and SD is 1. 

## Creating categorical variable of the crime rate

```{r Creating categorical variable}
# creating a quantile vector of crim 
bins <- quantile(boston_scaled$crim)

crime <- cut(boston_scaled$crim, breaks = bins, labels = c("low", "med_low", "med_high", "high"), include.lowest = TRUE)

table(crime)

#removing crim
boston_scaled <- dplyr::select(boston_scaled, -crim)

#adding categorical variable to the table
boston_scaled <- data.frame(boston_scaled, crime)

```

## Creating training set and set test
For predicting with data we need a model training set which is in this case decided to be 80% of the cases and the rest of the data is used as a test set which shows the accuracy of the model.

```{r training and test set}

n <- nrow(boston_scaled)

#Choosing 80% to the training set
ind <- sample(n,  size = n * 0.8)

train <- boston_scaled[ind,]

# creating the test set 
test <- boston_scaled[-ind,]


```

## Fitting linear discriminant analysis on the training set using crime rate as the target variable

```{r lda}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

```

## LDA biplot
```{r lda biplot}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
l <- plot(lda.fit, dimen = 2, col = classes, pch = classes)
l + lda.arrows(lda.fit, myscale = 1)

```
### Prediction based on the lda model

```{r testing the prediction}

# saving the correct classes from test data
correct_classes <- test$crime

# removing the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

From the cross table we can see that high values are predicted very nicely, but in the lower classes more errors occure. 

## Clustering, distances with euclidean distance

```{r}
# Boston dataset reading and standardization again

data("Boston")
b_boston_scaled <- scale(Boston)

# Distances with euclidean distance
dist_eu <- dist(b_boston_scaled)
summary(dist_eu)
```


## Clustering with K means with 3 cluster centers
```{r}
km <- kmeans(b_boston_scaled, centers = 3)
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(b_boston_scaled, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```
  
The optimal cluster size is the point where the line drops. In this it seems to be two.

```{r}
# Clustering again
km2 <- kmeans(b_boston_scaled, centers = 2)
pairs(b_boston_scaled[,1:7], col = km2$cluster)
pairs(b_boston_scaled[,8:14], col = km2$cluster)
```


### Bonus

```{r}
km3 <- kmeans(Boston, centers = 3)
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')

km4 <- kmeans(Boston, centers = 2)
pairs(Boston[,1:7], col = km4$cluster)
pairs(Boston[,8:14], col = km4$cluster)

bins <- quantile(Boston$crim)

crime2 <- cut(Boston$crim, breaks = bins, labels = c("low", "med_low", "med_high", "high"), include.lowest = TRUE)

table(crime2)

Boston <- dplyr::select(Boston, -crim)

Boston <- data.frame(Boston, crime2)

u <- nrow(Boston)

ind2 <- sample(u,  size = u * 0.8)

train2 <- Boston[ind2,]

test2 <- Boston[-ind2,]

lda.fit2 <- lda(crime2 ~ ., data = train2)

lda.fit2

lda.arrows2 <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes2 <- as.numeric(train2$crime2)

# plot the lda results
l <- plot(lda.fit2, dimen = 2, col = classes2, pch = classes2)
l + lda.arrows2(lda.fit2, myscale = 2)
```

Nox and seems to be the most influencal linear separators in analysis without standardization. 

# Super Bonus
I don´t get the colors right. Otherwise nice 
```{r}

model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = test$classes)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$clusters)

```
